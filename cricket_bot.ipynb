{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "import os\n",
    "\n",
    "api_key = os.getenv(\"NVIDIA_API_KEY\")\n",
    "os.environ[\"NVIDIA_API_KEY\"] = api_key\n",
    "print(api_key)\n",
    "# ChatNVIDIA.get_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful utility method for printing intermediate states\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def RPrint(preface=\"State: \"):\n",
    "    def print_and_return(x, preface=\"\"):\n",
    "        print(f\"{preface}{x}\")\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "\n",
    "def PPrint(preface=\"State: \"):\n",
    "    def print_and_return(x, preface=\"\"):\n",
    "        pprint(preface, x)\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cricket Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load documents from Wikipedia\n",
    "docs = WikipediaLoader(query=\"Cricket and everything related to cricket\",\n",
    "                       load_max_docs=1000).load()\n",
    "\n",
    "# Function to split text into chunks with headers\n",
    "\n",
    "\n",
    "def create_chunks_with_headers(doc, doc_index):\n",
    "    chunk_size = 1536\n",
    "    chunk_overlap = 200\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    doc_content = doc.page_content\n",
    "    doc_length = len(doc.page_content)\n",
    "\n",
    "    while start < doc_length:\n",
    "        end = min(start + chunk_size, doc_length)\n",
    "        chunk = doc_content[start:end]\n",
    "\n",
    "        if start != 0:\n",
    "            chunk = doc_content[max(start - chunk_overlap, 0):end]\n",
    "\n",
    "        chunk_json = {\n",
    "            \"meta_data\": {\n",
    "                \"title\": doc.metadata[\"title\"],\n",
    "                \"summary\": doc.metadata['summary'],\n",
    "                \"source_url\": doc.metadata['source'],\n",
    "            },\n",
    "            \"chunk_index\": len(chunks) + 1,\n",
    "            \"content\": chunk\n",
    "        }\n",
    "        chunks.append(chunk_json)\n",
    "\n",
    "        start += chunk_size\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Create an array to store all document chunks as JSON objects\n",
    "all_chunks = []\n",
    "\n",
    "# Create JSON objects for each document with chunks\n",
    "for i, doc in enumerate(docs):\n",
    "    chunks = create_chunks_with_headers(doc, i + 1)\n",
    "    all_chunks.extend(chunks)\n",
    "    print(f\"Data for document {i + 1} has been processed.\")\n",
    "\n",
    "# Output the array of JSON objects (for demonstration purposes)\n",
    "\n",
    "\n",
    "# If you want to write the array to a file:\n",
    "# with open('wikipedia_docs_chunks.json', 'w', encoding='utf-8') as file:\n",
    "#     json.dump(all_chunks, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"All data has been processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1377"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text splitting done into chunks with meta data for context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Query Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  lark langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain-cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Board of Control for Cricket in India', 'summary': \"Board of Control for Cricket in India (BCCI) is the national governing body of cricket in India. Its headquarters is situated at the cricket centre in Churchgate, Mumbai. The BCCI is the wealthiest governing body of cricket in the world.\\nThe BCCI was established in 1 December 1928 at Madras (currently Chennai) under Act XXI of 1860 of Madras and was subsequently reregistered under the Tamil Nadu Societies Registration Act, 1975. It is a consortium of state cricket associations that select their own representatives who elect the BCCI president. It joined the Imperial Cricket Conference in 1926 which later became the International Cricket Council. The BCCI is an autonomous, private organization that does not fall under the purview of the National Sports Federation of India of Government of India and does not receive any grants from the Ministry of Youth Affairs and Sports. The BCCI is influential in international cricket. The International Cricket Council shares the largest part of its revenue with the BCCI. IPL run by BCCI is one of the wealthiest sports leagues in the world. \\nIn financial year 2023-2024, BCCI earned ₹16,875 crore (US$2.0 billion). BCCI paid ₹4,000 crore (US$480 million) in taxes for the financial year 2022-23.\\nR. E. Grant Govan was the first BCCI president and Anthony De Mello was its first secretary. As of February 2023, Roger Binny is the incumbent BCCI president and Jay Shah is the secretary.\\nBCCI has hosted multiple Cricket World Cups, and will host the 2026 T20 World Cup, the 2029 Champions Trophy, the 2031 Cricket World Cup, and the 2025 Women's Cricket World Cup.\\nThe BCCI manages four squads that represent India in international cricket; the men's national cricket team, the women's national cricket team, the men's national under-19 cricket team and women's national under-19 cricket team. It also governs developmental India A, India B and India A women's teams. Its national selection committee, which is led by chief national selector, selects players for these teams. As part of its duties, the BCCI organises and schedules matches to be played by each of these teams, and schedules, sanctions and organises domestic cricket in India.\", 'source_url': 'https://en.wikipedia.org/wiki/Board_of_Control_for_Cricket_in_India'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "\n",
    "embeddings_model = CohereEmbeddings(\n",
    "    cohere_api_key=\"\")\n",
    "\n",
    "documents=[]\n",
    "\n",
    "for chunk in all_chunks:\n",
    "    doc = Document(\n",
    "            page_content=chunk[\"content\"],\n",
    "        metadata=chunk[\"meta_data\"],\n",
    "        )\n",
    "    documents.append(doc)\n",
    "\n",
    "print(documents[0].metadata)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents, embeddings_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our self-querying retriever\n",
    "Now we can instantiate our retriever. To do this we'll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"title\",\n",
    "        description=\"The name of the article\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"summary\",\n",
    "        description=\"The short summary of the article contents\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"source_url\",\n",
    "        description=\"The web uri link to the article webpage\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "]\n",
    "document_content_description = \"Data about cricket\"\n",
    "llm = ChatNVIDIA(model=\"mistralai/mistral-7b-instruct-v0.2\")\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# Initialize memory as a deque with a maximum length of 5\n",
    "memory = deque(maxlen=5)\n",
    "\n",
    "\n",
    "def update_memory(user_question, response):\n",
    "\n",
    "    memory.append({\n",
    "        \"question\": user_question,\n",
    "        \"response\": response,\n",
    "    })\n",
    "    pprint(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a pipeline for the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhiramyanampally/Downloads/NvidiaFrontend/.venv/lib/python3.12/site-packages/langchain_nvidia_ai_endpoints/_common.py:537: UserWarning: An API key is required for the hosted NIM. This will become an error in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'memory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Get user input and invoke the chain\u001b[39;00m\n\u001b[1;32m     82\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me more about his batting records\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 83\u001b[0m response \u001b[38;5;241m=\u001b[39m Runnable\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_input, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mmemory\u001b[49m})\n\u001b[1;32m     85\u001b[0m pprint(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'memory' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableAssign, RunnablePassthrough\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "sys_msg = \"\"\"\n",
    "You are an intelligent assistant that answers all questions about cricket using contextual information from Wikipedia. Your responses should be conversational and informative, providing clear and concise explanations. When relevant, include the source URL of the articles to give users additional reading material.\n",
    "\n",
    "Always aim to:\n",
    "1. Answer the question directly and clearly.\n",
    "2. Provide context and background information when useful but do not give irrelevant information and answer to the point.\n",
    "3. Suggest related topics or additional points of interest.\n",
    "4. Be polite and engaging in your responses.\n",
    "5. Remove the unnecessary context from the context provided if irrelevant to the question\n",
    "\n",
    "Now, let's get started!\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the chat model\n",
    "instruct_chat = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n",
    "llm = instruct_chat | StrOutputParser()\n",
    "\n",
    "\n",
    "def generate_embeddings(input_data):\n",
    "    embeddings = retriever.invoke(input_data)\n",
    "    if embeddings:\n",
    "        return embeddings\n",
    "    else:\n",
    "        return \"No data available\"\n",
    "\n",
    "\n",
    "def generate_embeddings_query(input_data):\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        f\"\"\"\n",
    "User's Question: {{input}}\n",
    "Previous conversation memory {{memory}}\n",
    "Generate only a query sentence and nothing else from the user's question to fetch from the data from embeddings. If the user's question does not have enough context then create a query based on the Knowledge Base.\n",
    "\"\"\"\n",
    "    )\n",
    "    embedding_chain = prompt | llm\n",
    "    embeddings_query = embedding_chain.invoke(input_data)\n",
    "    if embeddings_query:\n",
    "        return embeddings_query\n",
    "    else:\n",
    "        return \"Process failed\"\n",
    "\n",
    "\n",
    "generate_embeddings_runnable = RunnableLambda(generate_embeddings)\n",
    "generate_embeddings_query_runnable = RunnableLambda(generate_embeddings_query)\n",
    "\n",
    "\n",
    "\n",
    "def get_response(prompt):\n",
    "    return llm.invoke(prompt)\n",
    "\n",
    "\n",
    "# Create the Runnable chain with memory integration\n",
    "Runnable = (\n",
    "    {\"input\": RunnablePassthrough(), \"memory\": RunnablePassthrough()}\n",
    "    | RunnableAssign({\"embedding_query\": generate_embeddings_query_runnable})\n",
    "    | RunnableAssign({\"context\": generate_embeddings_runnable})\n",
    "    | RunnableAssign({\"prompt\": lambda x: ChatPromptTemplate.from_template(\n",
    "        f\"\"\"\n",
    "{sys_msg}\n",
    "\n",
    "User's Question: {{input}}\n",
    "\n",
    "Context Information: {{context}}\n",
    "\n",
    "Previous Conversation memory: {{memory}}\n",
    "\n",
    "Your Response:\n",
    "\"\"\"\n",
    "    )})\n",
    "    | RunnableAssign({\"response\": lambda x: get_response(x[\"prompt\"])})\n",
    "    | PPrint()\n",
    "    | RunnableAssign({\"memory\": lambda x: update_memory(x[\"input\"][\"input\"], x[\"response\"])})\n",
    ")\n",
    "\n",
    "# Get user input and invoke the chain\n",
    "user_input = \"Tell me more about his batting records\"\n",
    "response = Runnable.invoke({\"input\": user_input, \"memory\": memory})\n",
    "\n",
    "pprint(response[\"response\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
